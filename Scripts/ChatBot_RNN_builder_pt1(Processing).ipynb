{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The notebook for building an RNN from the chat data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning modules for building the module\n",
    "import keras \n",
    "\n",
    "# general modules for preprocessing and other stuff\n",
    "import os # for os related calls\n",
    "from subprocess import check_output\n",
    "\n",
    "# for visualization stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# for pickling the data\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print check_output(cmd).decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' define the important paths for the Task '''\n",
    "data_path = \"../Data/cornell_movie_dialogs_corpus\"\n",
    "train_file = os.path.join(data_path, \"movie_lines.txt\")\n",
    "\n",
    "''' define a few important constant --variables. '''\n",
    "field_separator = \"+++$+++\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with some preporcessing of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the file data into a list\n",
    "data = []\n",
    "with open(train_file, \"r\") as data_file:\n",
    "    for line in data_file:\n",
    "        data.append(line.strip())\n",
    "        \n",
    "# print a few data elements:\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip non-required information:\n",
    "data = map(lambda x: list(x.split(field_separator))[3:], data)\n",
    "\n",
    "# print some data fields:\n",
    "data[: 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now write a small loop to bring together adjacent dialogues into a single sentence:\n",
    "i = 0\n",
    "processed = []\n",
    "while(i < len(data)):\n",
    "    processed.append(data[i])\n",
    "    current = data[i][0] \n",
    "    \n",
    "    j = i + 1\n",
    "    while(j < len(data) and data[j][0] == current):\n",
    "        if(len(data[j]) == 2):\n",
    "            processed[-1][1] += data[j][1]\n",
    "        j += 1\n",
    "    \n",
    "    i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(processed)):\n",
    "    if(len(processed[i]) != 2):\n",
    "        print processed[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, there are around 2.7 lakhs of spoken sentences in this conversational dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the embeddings for the vocabulary of the words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the list into a numpy array:\n",
    "processed = np.array(processed, dtype=np.str)\n",
    "processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print type(processed) # check if it has been converted into a numpy array\n",
    "processed[3: 10] # print a few entries in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to keep the characters who spoke the sentences around anymore\n",
    "sentences = processed[:, 1]\n",
    "sentences.shape\n",
    "sentences[45: 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to build a formatted dataset from the list of words \n",
    "# so that the context of the words are taken into consideration\n",
    "\n",
    "def build_dataset(words):\n",
    "    import collections\n",
    "    \n",
    "    \"\"\" build a dataset from the list of words \n",
    "    \n",
    "        input: list of words\n",
    "        output: the formatted data, count, dictionary (map), reverse_dictionary\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary_size = len(words)  # size of the dictionary to be formed\n",
    "    \n",
    "    # form the dictionary\n",
    "    count = [] \n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "        \n",
    "    # create a list from the dictionary\n",
    "    data = list()\n",
    "    for word in words:       \n",
    "        index = dictionary[word]\n",
    "        data.append(index)\n",
    "  \n",
    "    # a reverse dictionary for mapping the words to their unique id\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a vocabulary of words. Use the above helper function for it.\n",
    "'''\n",
    "***********************************************************************\n",
    "WARNING! WARNING! WARNING! \n",
    "This cell may take some time to execute on some low end systems\n",
    "***********************************************************************\n",
    "'''\n",
    "\n",
    "# map and reduce is taking too long. So, I am trying to write a loop for it.\n",
    "words = list() # empty list\n",
    "for i in range(len(data)):\n",
    "    words += data[i][1].split()\n",
    "    \n",
    "print len(words) # print the length of the words\n",
    "words[100: 110] # print some arbitrary words from this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, use the words to build the dataset.\n",
    "data , count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "\n",
    "# add the special characters to the list manually:\n",
    "size = len(dictionary)\n",
    "dictionary[\" \"] = size # blank space character.\n",
    "dictionary[\"other\"] = size + 1\n",
    "\n",
    "reverse_dictionary[size] = \" \"\n",
    "reverse_dictionary[size + 1] = \"other\"\n",
    "\n",
    "vocab_size = len(dictionary) # size of the vocabulary\n",
    "print \"Vocabulary_size: \" + str(vocab_size)\n",
    "\n",
    "# print a few items from all of these\n",
    "print \"DATA               : \" + str(data[:10])\n",
    "print \"COUNT              : \" + str(count[:10])\n",
    "print \"DICTIONARY         : \" + str((dictionary.keys()[:10], dictionary.values()[: 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now transform the sentences into formatted sequences for the chatbot\n",
    "sentences = map(lambda x: x.split(), sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "***********************************************************************\n",
    "WARNING! WARNING! WARNING! \n",
    "This cell may take some time to execute on some low end systems\n",
    "***********************************************************************\n",
    "'''\n",
    "\n",
    "# set the input Dim constant here:\n",
    "inputDim = 35\n",
    "\n",
    "# loop through the sentences to make their lengths equal to the inputDim\n",
    "\n",
    "fixed_length_input = list()\n",
    "for i in range(len(sentences)):\n",
    "    sentence = sentences[i] # extract the sentence\n",
    "    \n",
    "    if(len(sentence) < inputDim):\n",
    "        while(len(sentence) != inputDim):\n",
    "            sentence.append(\" \") # append the blank character\n",
    "        fixed_length_input.append(sentence)\n",
    "        \n",
    "    else:\n",
    "        # The length is greater than or equal to 35\n",
    "        splits = list()\n",
    "        for j in range(len(sentence)):\n",
    "            if(j % inputDim == 0):\n",
    "                splits.append(list()) # append an empty list\n",
    "                \n",
    "            splits[-1].append(sentence[j])\n",
    "\n",
    "        # pad the last list with the appropriate blanks\n",
    "        while(len(splits[-1]) != inputDim):\n",
    "            splits[-1].append(\" \")\n",
    "\n",
    "        # concat the sentences and the splits\n",
    "        fixed_length_input = fixed_length_input + splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# length checking:\n",
    "lengths = map(lambda x: len(x), fixed_length_input)\n",
    "\n",
    "for i in range(len(lengths)):\n",
    "    if len(fixed_length_input[i]) != inputDim:\n",
    "        print fixed_length_input[i]\n",
    "        \n",
    "# The check has passed since this didnot print anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty ndArray\n",
    "Data = np.ndarray((len(fixed_length_input), inputDim), dtype = np.int32)\n",
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now fill the Data using the dictionary mapping:\n",
    "for i in range(len(fixed_length_input)):\n",
    "    for j in range(inputDim):\n",
    "        Data[i, j] = dictionary[fixed_length_input[i][j]]\n",
    "        \n",
    "# print a few values from the Data\n",
    "print Data[5: 10, :]\n",
    "\n",
    "# print a random sentence \n",
    "reduce(lambda x, y: x + \" \" + y, map(lambda x: reverse_dictionary[x], Data[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of the saved Data and pickle this processed Data\n",
    "Processed_Data = {\n",
    "    \"data\": Data,\n",
    "    \"mapping\": dictionary,\n",
    "    \"rev_mapping\": reverse_dictionary\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now pickle the Processed_Data Dictionary\n",
    "save_path = os.path.join(data_path, \"Data_final.pickle\")\n",
    "\n",
    "if(not os.path.isfile(save_path)):\n",
    "    with open(save_path, \"wb\") as pickle_file:\n",
    "        pickle.dump(Processed_Data, pickle_file, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Pickling complete\")\n",
    "        \n",
    "else:\n",
    "    print(\"data is already pickled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# naive checking of the integrity of the pickled data\n",
    "\n",
    "with open(save_path, \"rb\") as pickle_file:\n",
    "    my_dict = pickle.load(pickle_file)\n",
    "    \n",
    "len(my_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data has been processed and pickled so that the Next part can be directly run the next time I come here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
