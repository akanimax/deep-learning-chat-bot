{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The notebook for building an RNN from the chat data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning modules for building the module\n",
    "import keras \n",
    "\n",
    "# general modules for preprocessing and other stuff\n",
    "import os # for os related calls\n",
    "from subprocess import check_output\n",
    "\n",
    "# for visualization stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# for pickling the data\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print check_output(cmd).decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' define the important paths for the Task '''\n",
    "data_path = \"../Data/cornell_movie_dialogs_corpus\"\n",
    "train_file = os.path.join(data_path, \"movie_lines.txt\")\n",
    "\n",
    "''' define a few important constant --variables. '''\n",
    "field_separator = \"+++$+++\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with some preporcessing of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
       " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
       " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the file data into a list\n",
    "data = []\n",
    "with open(train_file, \"r\") as data_file:\n",
    "    for line in data_file:\n",
    "        data.append(line.strip())\n",
    "        \n",
    "# print a few data elements:\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' BIANCA ', ' They do not!'],\n",
       " [' CAMERON ', ' They do to!'],\n",
       " [' BIANCA ', ' I hope so.']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strip non-required information:\n",
    "data = map(lambda x: list(x.split(field_separator))[3:], data)\n",
    "\n",
    "# print some data fields:\n",
    "data[: 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now write a small loop to bring together adjacent dialogues into a single sentence:\n",
    "i = 0\n",
    "processed = []\n",
    "while(i < len(data)):\n",
    "    processed.append(data[i])\n",
    "    current = data[i][0] \n",
    "    \n",
    "    j = i + 1\n",
    "    while(j < len(data) and data[j][0] == current):\n",
    "        if(len(data[j]) == 2):\n",
    "            processed[-1][1] += data[j][1]\n",
    "        j += 1\n",
    "    \n",
    "    i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' BIANCA ', ' They do not!'], [' CAMERON ', ' They do to!']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(processed)):\n",
    "    if(len(processed[i]) != 2):\n",
    "        print processed[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, there are around 2.7 lakhs of spoken sentences in this conversational dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a BAG OF WORDS for the vocabulary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(269955, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the list into a numpy array:\n",
    "processed = np.array(processed, dtype=np.str)\n",
    "processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[' CAMERON ', ' She okay?'],\n",
       "       [' BIANCA ', \" Let's go.\"],\n",
       "       [' CAMERON ', ' Wow'],\n",
       "       [' BIANCA ', \" Okay -- you're gonna need to learn how to lie.\"],\n",
       "       [' CAMERON ', ' No'],\n",
       "       [' BIANCA ',\n",
       "        ' I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit? Like my fear of wearing pastels?'],\n",
       "       [' CAMERON ', ' The \"real you\".']],\n",
       "      dtype='|S3089')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print type(processed) # check if it has been converted into a numpy array\n",
    "processed[3: 10] # print a few entries in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\" Right.  See?  You're ready for the quiz.\",\n",
       "       \" C'esc ma tete. This is my head\", ' Let me see what I can do.',\n",
       "       ' Gosh, if only we could find Kat a boyfriend...',\n",
       "       \" That's a shame.\"],\n",
       "      dtype='|S3089')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no need to keep the characters who spoke the sentences around anymore\n",
    "sentences = processed[:, 1]\n",
    "sentences.shape\n",
    "sentences[45: 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to build a formatted dataset from the list of words \n",
    "# so that the context of the words are taken into consideration\n",
    "\n",
    "def build_dataset(words):\n",
    "    import collections\n",
    "    \n",
    "    \"\"\" build a dataset from the list of words \n",
    "    \n",
    "        input: list of words\n",
    "        output: the formatted data, count, dictionary (map), reverse_dictionary\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary_size = 100000  # size of the dictionary to be formed\n",
    "    \n",
    "    # form the dictionary\n",
    "    count = [] \n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    # add some details here:\n",
    "    dictionary[\" \"] = size # blank space character.\n",
    "    dictionary[\"<other>\"] = size + 1\n",
    "    \n",
    "        \n",
    "    # create a list from the dictionary\n",
    "    data = list()\n",
    "    for word in words:\n",
    "        try:\n",
    "            index = dictionary[word]\n",
    "        except KeyError:\n",
    "            index = dictionary['<other>']\n",
    "        data.append(index)\n",
    "  \n",
    "    # a reverse dictionary for mapping the words to their unique id\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3605023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['babble.',\n",
       " \"i'm\",\n",
       " 'like,',\n",
       " 'boring',\n",
       " 'myself.',\n",
       " 'what',\n",
       " 'crap?',\n",
       " 'do',\n",
       " 'you',\n",
       " 'listen']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a vocabulary of words. Use the above helper function for it.\n",
    "'''\n",
    "***********************************************************************\n",
    "WARNING! WARNING! WARNING! \n",
    "This cell may take some time to execute on some low end systems\n",
    "***********************************************************************\n",
    "'''\n",
    "\n",
    "# map and reduce is taking too long. So, I am trying to write a loop for it.\n",
    "words = list() # empty list\n",
    "for i in range(len(data)):\n",
    "    words += map(lambda x: x.lower(), data[i][1].split())\n",
    "    \n",
    "print len(words) # print the length of the words All the possible words\n",
    "# words = list(set(words)) # remove all the duplicate words from the set\n",
    "# print len(words)\n",
    "words[100: 110] # print some arbitrary words from this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3605023\n",
      "Vocabulary_size: 100002\n",
      "DATA               : [40, 23, 2106, 40, 23, 3818, 1, 376, 387, 54]\n",
      "COUNT              : [('you', 118207), ('i', 111794), ('the', 110671), ('to', 88125), ('a', 79057), ('and', 50048), ('of', 43704), ('in', 36122), ('it', 32991), ('that', 30057)]\n",
      "DICTIONARY         : (['fawn', 'considered,', 'petra,', 'considered.', 'body-guard.', 'simple-icity', 'this--\"look', '$750.00', 'petra?', 'considered?'], [73158, 47074, 36554, 73159, 73160, 90655, 73161, 84699, 73164, 73165])\n"
     ]
    }
   ],
   "source": [
    "# Now, use the words to build the dataset.\n",
    "print len(words)\n",
    "data , count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "\n",
    "# add the special characters to the list manually:\n",
    "size = len(dictionary)\n",
    "\n",
    "vocab_size = len(dictionary) # size of the vocabulary\n",
    "print \"Vocabulary_size: \" + str(vocab_size)\n",
    "\n",
    "# print a few items from all of these\n",
    "print \"DATA               : \" + str(data[:10])\n",
    "print \"COUNT              : \" + str(count[:10])\n",
    "print \"DICTIONARY         : \" + str((dictionary.keys()[:10], dictionary.values()[: 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary[100002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now transform the sentences into formatted sequences for the chatbot\n",
    "sentences = map(lambda x: x.lower().split(), sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "***********************************************************************\n",
    "WARNING! WARNING! WARNING! \n",
    "This cell may take some time to execute on some low end systems\n",
    "***********************************************************************\n",
    "'''\n",
    "\n",
    "# set the input Dim constant here:\n",
    "inputDim = 35\n",
    "\n",
    "# loop through the sentences to make their lengths equal to the inputDim\n",
    "\n",
    "fixed_length_input = list()\n",
    "for i in range(len(sentences)):\n",
    "    sentence = sentences[i] # extract the sentence\n",
    "    \n",
    "    if(len(sentence) < inputDim):\n",
    "        while(len(sentence) != inputDim):\n",
    "            sentence.append(\" \") # append the blank character\n",
    "        fixed_length_input.append(sentence)\n",
    "        \n",
    "    else:\n",
    "        # The length is greater than or equal to 35\n",
    "        splits = list()\n",
    "        for j in range(len(sentence)):\n",
    "            if(j % inputDim == 0):\n",
    "                splits.append(list()) # append an empty list\n",
    "                \n",
    "            splits[-1].append(sentence[j])\n",
    "\n",
    "        # pad the last list with the appropriate blanks\n",
    "        while(len(splits[-1]) != inputDim):\n",
    "            splits[-1].append(\" \")\n",
    "\n",
    "        # concat the sentences and the splits\n",
    "        fixed_length_input = fixed_length_input + splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# length checking:\n",
    "lengths = map(lambda x: len(x), fixed_length_input)\n",
    "\n",
    "for i in range(len(lengths)):\n",
    "    if len(fixed_length_input[i]) != inputDim:\n",
    "        print fixed_length_input[i]\n",
    "        \n",
    "# The check has passed since this didnot print anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286730, 35)\n"
     ]
    }
   ],
   "source": [
    "# create an empty ndArray\n",
    "Data = np.ndarray((len(fixed_length_input), inputDim), dtype = np.int32)\n",
    "print Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i'm not stupid enough to repeat your mistakes.                                                      \""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now fill the Data using the dictionary mapping:\n",
    "for i in range(len(fixed_length_input)):\n",
    "    for j in range(inputDim):\n",
    "        try: \n",
    "            Data[i, j] = dictionary[fixed_length_input[i][j]]\n",
    "        except KeyError:\n",
    "            Data[i, j] = dictionary[\"<other>\"]\n",
    "        \n",
    "# print a few values from the Data\n",
    "# print Data[5: 10, :]\n",
    "\n",
    "# print a random sentence \n",
    "reduce(lambda x, y: x + \" \" + y, map(lambda x: reverse_dictionary[x], Data[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Sentence: 1: no, they wouldn't -- they'd gain one! and i guarantee that they'll graduate with highest <other>                                      \n",
      "\n",
      "Random Sentence: 2: what's that?                                                                  \n",
      "\n",
      "Random Sentence: 3: hey -- so i've noticed.                                                            \n",
      "\n",
      "Random Sentence: 4: we had time to get to know each other.                                                    \n",
      "\n",
      "Random Sentence: 5: the same reason everyone does. you hear your name on <other> and you realize you're a skeleton in someone's closet and they're coming to bury you.                  \n",
      "\n",
      "Random Sentence: 6: eighteen.                                                                    \n",
      "\n",
      "Random Sentence: 7: you can't be serious.                                                              \n",
      "\n",
      "Random Sentence: 8: remember the time we broke into the d.a.'s office, and copied <other> <other> diary?                                          \n",
      "\n",
      "Random Sentence: 9: do you have a fella?                                                            \n",
      "\n",
      "Random Sentence: 10: jack.                                                                    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print a random sentence \n",
    "for i in range(10):\n",
    "    print \"Random Sentence: \" + str(i + 1) + \": \" + reduce(\n",
    "        lambda x, y: x + \" \" + y, map(lambda x: reverse_dictionary[x], Data[np.random.randint(len(Data))])) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of the saved Data and pickle this processed Data\n",
    "Processed_Data = {\n",
    "    \"data\": Data,\n",
    "    \"mapping\": dictionary,\n",
    "    \"rev_mapping\": reverse_dictionary\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data is already pickled\n"
     ]
    }
   ],
   "source": [
    "# Now pickle the Processed_Data Dictionary\n",
    "save_path = os.path.join(data_path, \"Data_final.pickle\")\n",
    "\n",
    "if(not os.path.isfile(save_path)):\n",
    "    with open(save_path, \"wb\") as pickle_file:\n",
    "        pickle.dump(Processed_Data, pickle_file, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Pickling complete\")\n",
    "        \n",
    "else:\n",
    "    print(\"data is already pickled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# naive checking of the integrity of the pickled data\n",
    "\n",
    "with open(save_path, \"rb\") as pickle_file:\n",
    "    my_dict = pickle.load(pickle_file)\n",
    "    \n",
    "len(my_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data has been processed and pickled so that the Next part can be directly run the next time I come here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
